# Traitement_images_avec_deep_Learnig(CNN)
üìù Contexte du projet 
La reconnaissance faciale est aujourd‚Äôhui au c≈ìur de nombreuses applications modernes : s√©curit√©, biom√©trie, r√©seaux sociaux, sant√©, ou encore commerce. Toutefois, reconna√Ætre un visage en conditions r√©elles reste un d√©fi de taille, en raison des variations d‚Äô√©clairage, de pose, d'expression, d'√¢ge ou encore de qualit√© d‚Äôimage.

Dans ce projet, nous explorons l‚Äôutilisation de techniques de deep learning pour aborder le probl√®me de la reconnaissance faciale √† partir du jeu de donn√©es Labeled Faces in the Wild (LFW). Ce dataset, devenu un standard dans la recherche, propose plus de 13 000 images de visages collect√©es sur Internet, et annot√©es avec les noms des personnes photographi√©es.

Les images ont √©t√© d√©tect√©es √† l‚Äôaide du d√©tecteur de visages de Viola-Jones, et pr√©sentent des conditions vari√©es et non contr√¥l√©es ‚Äî ce qui refl√®te les difficult√©s d‚Äôune application en environnement r√©el. Le dataset comporte 1 680 personnes ayant au moins deux images diff√©rentes, ce qui le rend particuli√®rement adapt√© √† l‚Äôentra√Ænement de mod√®les de reconnaissance de visages √† l‚Äôaide de r√©seaux de neurones convolutifs (CNN).

Notre objectif dans ce projet est de :

Pr√©traiter et organiser les images du dataset,

Entra√Æner un mod√®le de classification (ou de v√©rification) de visages,

√âvaluer ses performances sur un ensemble de test non vu,

√âtudier les limites et axes d‚Äôam√©lioration d‚Äôun tel syst√®me.

R√©sum√© des essais r√©alis√©s
Premier test : CNN simple

Mod√®le basique avec 2 couches convolutionnelles, 1 couche dense finale.

Images redimensionn√©es √† 64x64 pixels.

R√©sultat : environ 15 % de pr√©cision sur le test.

Limites : mod√®le trop simple, peu d‚Äôimages par classe, pas d‚Äôaugmentation des donn√©es, taille d‚Äôimage faible.

Deuxi√®me test : CNN am√©lior√© + augmentation de donn√©es

Ajout d‚Äôune couche convolutionnelle suppl√©mentaire, dropout pour √©viter le sur-apprentissage.

Augmentation de donn√©es (rotation, zoom, translation, flip).

Seules les personnes avec au moins 3 images ont √©t√© conserv√©es.

R√©sultat : environ 21 % de pr√©cision.

Am√©lioration notable, mais encore insuffisant pour une bonne reconnaissance faciale.

Troisi√®me test : transfert learning avec MobileNetV2 pr√©-entra√Æn√©

Images redimensionn√©es √† 160x160 pixels pour correspondre √† l‚Äôentr√©e du mod√®le MobileNetV2.

Filtrage plus strict : minimum 15 images par personne pour un meilleur apprentissage.

Couche de base gel√©e, ajout d‚Äôun classifieur personnalis√© avec dropout.

R√©sultat : environ 54 % de pr√©cision.

Conclusion : le transfert learning apporte un vrai gain en performance gr√¢ce √† l‚Äôutilisation de repr√©sentations visuelles d√©j√† apprises sur un large corpus d‚Äôimages.

Points cl√©s √† retenir
Qualit√© des donn√©es : avoir suffisamment d‚Äôexemples par personne est crucial.

Augmentation des donn√©es : n√©cessaire pour am√©liorer la robustesse du mod√®le.

Taille d‚Äôimage : plus grande, plus d‚Äôinformations et meilleures performances.

Transfert learning : m√©thode efficace pour exploiter des mod√®les pr√©-entra√Æn√©s sur de larges bases et les adapter √† des t√¢ches sp√©cifiques.
